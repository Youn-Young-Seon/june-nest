---
description: 
globs: 
alwaysApply: true
---
# FxTS Functional Programming Guidelines

## Overview
This project uses [FxTS](mdc:https:/fxts.dev/docs) for functional programming patterns in TypeScript/JavaScript. FxTS provides two main categories of functions: **Lazy** (for lazy evaluation and iterators) and **Strict** (for strict evaluation on arrays and objects).

## ğŸš¨ CRITICAL: Understanding Lazy Evaluation and Iterator Protocol

### âš ï¸ Key Concept: Iterator vs Array Returns
**FxTS functions return Iterators, NOT arrays.** This is crucial for avoiding type errors.

```typescript
// âŒ WRONG: Assuming map returns an array
const result = pipe(
  users,
  map(user => ({ ...user, processed: true })),
  // This is an Iterator, not an array!
);

// âœ… CORRECT: Understanding Iterator flow
const result = pipe(
  users,
  map(user => ({ ...user, processed: true })),
  toArray  // Converts Iterator to Array (may return Promise<T[]>!)
);
```

### ğŸ”‘ When toArray Returns Promise<T[]>
The `toArray` function can return either `T[]` or `Promise<T[]>` depending on pipeline complexity:

```typescript
// Simple case - returns T[]
const result = pipe([1, 2, 3], map(x => x * 2), toArray);

// Complex case - returns Promise<T[]>  
const result = pipe(
  largeDataset,
  filter(item => expensiveCheck(item)),
  map(item => transform(item)),
  toArray  // âš ï¸ This may return Promise<T[]>!
);
```

## Function Categories

### Lazy Functions
Use lazy functions for:
- Large datasets that don't need to be fully processed
- Chaining operations that may not need all results
- Memory-efficient data processing
- Infinite or very large sequences

Common lazy functions:
- `map`, `filter`, `take`, `drop`, `chunk`, `flatten`, `uniq`
- `range`, `repeat`, `cycle` for sequence generation
- `concurrent`, `concurrentPool` for async operations

### Strict Functions
Use strict functions for:
- Small to medium datasets
- Operations that need immediate results
- Final transformations in a pipeline
- Simple data queries

Common strict functions:
- `reduce`, `find`, `some`, `every`, `includes`
- `groupBy`, `countBy`, `partition`, `sortBy`
- `pick`, `omit`, `prop`, `props` for object manipulation

## ğŸš€ ESSENTIAL: Asynchronous Pipeline Best Practices

### ğŸ’¡ Rule #1: Use toAsync for Complex Pipelines
When working with complex transformations or when `toArray` returns a Promise, **always use `toAsync`**:

```typescript
// âœ… BEST PRACTICE: Explicit async pipeline
const processedUsers = await pipe(
  users,
  toAsync,  // ğŸ”‘ KEY: Convert to AsyncIterable
  filter(user => user.isActive),
  map(user => ({
    ...user,
    displayName: user.name || user.email,
    videoCount: user.Video.length
  })),
  sortBy(user => user.createdAt),
  take(10),
  toArray   // Now safely returns Promise<T[]>
);
```

### ğŸ’¡ Rule #2: Always Await Async Pipelines
When using `toAsync` or when pipeline returns Promise, **always use `await`**:

```typescript
// âŒ WRONG: Not awaiting Promise
const result = pipe(
  data,
  toAsync,
  map(item => process(item)),
  toArray,
  items => items.filter(...)  // ERROR: items is Promise, not array
);

// âœ… CORRECT: Proper await handling
const processedItems = await pipe(
  data,
  toAsync,
  map(item => process(item)),
  toArray
);
const result = processedItems.filter(...);  // Now items is actual array
```

### ğŸ’¡ Rule #3: Separate Transformation and Post-Processing
For better type safety and readability:

```typescript
// âœ… BEST PRACTICE: Clear separation
const transformedData = await pipe(
  rawData,
  toAsync,
  filter(isValid),
  map(transform),
  sortBy(item => item.priority),
  toArray
);

// Now use regular array methods safely
const finalResult = transformedData
  .slice(0, 10)
  .map(item => ({ ...item, processed: true }));
```

## Pipeline Composition

### Preferred Pattern: Use `pipe` for Data Transformation
```typescript
// âœ… GOOD: Clear data transformation pipeline
const processUsers = (users: User[]) =>
  pipe(
    users,
    filter(user => user.isActive),
    map(user => ({ ...user, displayName: `${user.firstName} ${user.lastName}` })),
    sortBy(user => user.createdAt),
    take(10)
  );
```

### Use `pipeLazy` for Lazy Evaluation
```typescript
// âœ… GOOD: Lazy pipeline for large datasets
const processLargeDataset = (data: Iterable<Item>) =>
  pipeLazy(
    data,
    filter(item => item.isValid),
    map(item => transform(item)),
    chunk(100),
    take(5) // Only process first 5 chunks
  );
```

### Avoid Nested Function Calls
```typescript
// âŒ BAD: Hard to read nested calls
const result = take(10, 
  sortBy(user => user.createdAt,
    map(user => ({ ...user, displayName: `${user.firstName} ${user.lastName}` }),
      filter(user => user.isActive, users)
    )
  )
);

// âœ… GOOD: Use pipe for readability
const result = pipe(
  users,
  filter(user => user.isActive),
  map(user => ({ ...user, displayName: `${user.firstName} ${user.lastName}` })),
  sortBy(user => user.createdAt),
  take(10)
);
```

## Data Transformation Best Practices

### Object Manipulation
```typescript
// âœ… GOOD: Use FxTS object utilities
const userSummary = pipe(
  user,
  pick(['id', 'name', 'email']),
  evolve({
    name: name => name.toUpperCase(),
    email: email => email.toLowerCase()
  })
);

// âœ… GOOD: Safe property access
const userName = prop('name', user);
const userProps = props(['name', 'email'], user);
```

### Array Processing
```typescript
// âœ… GOOD: Use appropriate functions for array operations
const activeUsers = pipe(
  users,
  filter(user => user.isActive),
  uniqBy(user => user.email), // Remove duplicates by email
  sortBy(user => user.lastName)
);

// âœ… GOOD: Use reduce for aggregations
const userStats = pipe(
  users,
  reduce((acc, user) => ({
    total: acc.total + 1,
    active: acc.active + (user.isActive ? 1 : 0)
  }), { total: 0, active: 0 })
);
```

### Conditional Logic
```typescript
// âœ… GOOD: Use functional conditional utilities
const processUser = (user: User) => pipe(
  user,
  when(user => user.isNewUser, evolve({ status: () => 'pending' })),
  unless(user => user.isVerified, omit(['sensitiveData']))
);
```

## Performance Considerations

### When to Use toAsync
Use `toAsync` in these critical scenarios:

1. **Complex transformations** that might involve async operations
2. **Large datasets** where lazy evaluation is beneficial  
3. **When toArray returns Promise** (detected through type errors)
4. **Service layer methods** for consistent async handling

```typescript
// âœ… GOOD: Complex transformation pipeline
const analytics = await pipe(
  users,
  toAsync,  // Handle complex processing
  filter(user => user.isActive),
  map(async user => ({
    ...user,
    stats: await calculateUserStats(user)  // Async operation
  })),
  sortBy(user => user.stats.score),
  take(100),
  toArray
);
```

### Memory Efficiency with Lazy Evaluation
```typescript
// âœ… GOOD: Memory-efficient processing
const topUsers = await pipe(
  millionsOfUsers,
  toAsync,
  filter(user => user.isActive),
  sortBy(user => user.score),
  take(10),  // Only process top 10, not all millions
  toArray
);
```

### Lazy vs Strict Choice
```typescript
// âœ… GOOD: Use lazy for large datasets or partial processing
const firstActiveUser = pipe(
  users, // Large array
  filter(user => user.isActive), // Lazy - stops when first match found
  take(1), // Only need first result
  toArray // Convert to array at the end
);

// âœ… GOOD: Use strict for small datasets or complete processing
const userCount = pipe(
  users, // Small array, need all results
  filter(user => user.isActive),
  size // Need complete count
);
```

### Concurrent Processing
```typescript
// âœ… GOOD: Use concurrent for async operations
const enrichedUsers = await pipe(
  users,
  toAsync,  // Always use toAsync with concurrent
  concurrent(async user => ({
    ...user,
    profile: await fetchUserProfile(user.id)
  })),
  toArray
);

// âœ… GOOD: Use concurrentPool for rate limiting
const processedUsers = await pipe(
  users,
  toAsync,  // Always use toAsync with concurrentPool
  concurrentPool(5, async user => await processUser(user)), // Max 5 concurrent
  toArray
);
```

## Type Safety Guidelines

### Generic Functions
```typescript
// âœ… GOOD: Explicit type annotations for complex transformations
const transformUsers = <T extends User>(users: T[]): UserSummary[] =>
  pipe(
    users,
    map((user: T): UserSummary => ({
      id: user.id,
      name: user.name,
      isActive: user.isActive
    }))
  );
```

### Utility Functions
```typescript
// âœ… GOOD: Use FxTS type guards
const validateUser = (user: unknown): user is User => 
  isObject(user) && 
  isString(prop('name', user)) && 
  isBoolean(prop('isActive', user));

// âœ… GOOD: Use isEmpty for null/undefined checks
const hasValidEmail = (user: User): boolean => 
  !isEmpty(user.email) && isString(user.email);
```

## Common Patterns

### Data Aggregation
```typescript
// âœ… GOOD: Group and aggregate data
const usersByDepartment = pipe(
  users,
  groupBy(user => user.department),
  map(users => ({
    count: size(users),
    activeCount: pipe(users, filter(user => user.isActive), size)
  }))
);
```

### Error Handling
```typescript
// âœ… GOOD: Use throwIf for validation
const validateAndProcess = (data: unknown) => pipe(
  data,
  throwIf(isEmpty, 'Data cannot be empty'),
  throwIf(data => !isArray(data), 'Data must be an array'),
  map(processItem)
);
```

### Memoization
```typescript
// âœ… GOOD: Use memoize for expensive computations
const expensiveComputation = memoize((input: string) => {
  // Expensive operation
  return computeResult(input);
});
```

## ğŸš¨ Critical Anti-Patterns to Avoid

### âŒ Anti-Pattern #1: Ignoring Promise Returns
```typescript
// âŒ BAD: Ignoring that toArray might return Promise
const result = pipe(data, map(...), toArray);
return result.length;  // ERROR if result is Promise

// âœ… GOOD: Handling Promise correctly
const result = await pipe(data, toAsync, map(...), toArray);
return result.length;
```

### âŒ Anti-Pattern #2: Mixing Sync/Async Inconsistently
```typescript
// âŒ BAD: Inconsistent async handling
const part1 = pipe(data, map(...), toArray);  // Might be Promise
const part2 = data.filter(...);  // Always array
return [...part1, ...part2];  // ERROR if part1 is Promise

// âœ… GOOD: Consistent async handling
const part1 = await pipe(data, toAsync, map(...), toArray);
const part2 = data.filter(...);
return [...part1, ...part2];
```

### âŒ Anti-Pattern #3: Not Using toAsync When Needed
```typescript
// âŒ BAD: Complex pipeline without toAsync
const result = pipe(
  complexData,
  filter(expensiveCheck),
  map(heavyTransform),
  toArray,
  items => items.slice(0, 10)  // ERROR: items might be Promise
);

// âœ… GOOD: Explicit async handling
const items = await pipe(
  complexData,
  toAsync,
  filter(expensiveCheck),
  map(heavyTransform),
  toArray
);
const result = items.slice(0, 10);
```

### âŒ Anti-Pattern #4: Mixing Paradigms
```typescript
// âŒ BAD: Mixing imperative and functional styles
const processUsers = (users: User[]) => {
  const filtered = filter(user => user.isActive, users);
  const result = [];
  for (const user of filtered) {
    result.push({ ...user, processed: true });
  }
  return result;
};

// âœ… GOOD: Pure functional approach
const processUsers = async (users: User[]) => await pipe(
  users,
  toAsync,
  filter(user => user.isActive),
  map(user => ({ ...user, processed: true })),
  toArray
);
```

### âŒ Anti-Pattern #5: Ignoring Lazy Evaluation Benefits
```typescript
// âŒ BAD: Converting to array too early
const result = pipe(
  largeDataset,
  toArray, // Loses lazy evaluation benefits
  filter(item => item.isValid),
  take(10)
);

// âœ… GOOD: Keep lazy until the end
const result = await pipe(
  largeDataset,
  toAsync,
  filter(item => item.isValid),
  take(10),
  toArray // Convert only when needed
);
```

## Integration with NestJS

### Service Layer Best Practices
```typescript
// âœ… EXCELLENT: Proper async FxTS in service
@Injectable()
export class UserService {
  async getActiveUsers(): Promise<UserSummary[]> {
    const users = await this.prisma.user.findMany({ ... });
    
    return await pipe(
      users,
      toAsync,  // ğŸ”‘ Always use toAsync for service methods
      filter(user => user.isActive),
      map(user => this.toUserSummary(user)),
      sortBy(user => user.createdAt),
      take(10),
      toArray
    );
  }
  
  async getUserAnalytics(): Promise<UserAnalytics> {
    const users = await this.prisma.user.findMany({ ... });
    
    const [activeUsers, usersByRole, topUploaders] = await Promise.all([
      pipe(
        users,
        toAsync,
        filter(user => user.isActive),
        toArray
      ),
      pipe(
        users,
        toAsync,
        groupBy(user => user.role),
        toArray
      ),
      pipe(
        users,
        toAsync,
        sortBy(user => user.Video.length),
        take(5),
        toArray
      )
    ]);
    
    return { activeUsers, usersByRole, topUploaders };
  }
}
```

### Controller Layer
```typescript
// âœ… GOOD: Transform data in controllers
@Controller('users')
export class UserController {
  @Get()
  async getUsers(@Query() query: GetUsersQuery) {
    const users = await this.userService.findUsers(query);
    
    return await pipe(
      users,
      toAsync,  // Use toAsync in controllers too
      map(user => omit(['password', 'internalId'], user)),
      when(() => query.includeInactive, identity),
      unless(() => query.includeInactive, filter(user => user.isActive)),
      toArray
    );
  }
}
```

### Error Handling in Services
```typescript
@Injectable()
export class DataService {
  async processData(data: DataInput[]): Promise<ProcessedData[]> {
    try {
      return await pipe(
        data,
        toAsync,
        filter(this.isValid),
        map(async item => await this.transform(item)),
        toArray
      );
    } catch (error) {
      this.logger.error('FxTS pipeline failed:', error);
      throw new InternalServerErrorException('Data processing failed');
    }
  }
}

## Testing with FxTS

### Unit Tests for Async Pipelines
```typescript
// âœ… EXCELLENT: Test async FxTS pipelines
describe('UserService', () => {
  it('should process users correctly', async () => {
    const users = [
      { id: 1, name: 'John', isActive: true },
      { id: 2, name: 'Jane', isActive: false }
    ];
    
    const result = await pipe(
      users,
      toAsync,  // Use toAsync in tests too
      filter(user => user.isActive),
      map(user => user.name),
      toArray
    );
    
    expect(result).toEqual(['John']);
  });
  
  it('should handle empty input', async () => {
    const result = await pipe(
      [],
      toAsync,
      map(x => x * 2),
      toArray
    );
    
    expect(result).toEqual([]);
  });
  
  it('should handle errors properly', async () => {
    const users = [{ id: 1, name: 'John' }];
    
    await expect(
      pipe(
        users,
        toAsync,
        map(user => { throw new Error('Test error'); }),
        toArray
      )
    ).rejects.toThrow('Test error');
  });
});
```

### Integration Tests
```typescript
// âœ… GOOD: Test service integration
describe('UserService Integration', () => {
  it('should get analytics correctly', async () => {
    const service = new UserService(mockPrisma);
    
    const analytics = await service.getUserAnalytics();
    
    expect(analytics.activeUsers).toBeDefined();
    expect(analytics.usersByRole).toBeDefined();
    expect(analytics.topUploaders).toBeDefined();
  });
});
```

## ğŸ“‹ Quick Reference Summary

### ğŸ”‘ Key Rules to Remember:
1. **Always use `toAsync`** for complex pipelines and service methods
2. **Always `await`** when using `toAsync` or when `toArray` might return Promise
3. **Separate FxTS transformations** from regular array operations
4. **Handle type errors** by understanding Iterator vs Array returns
5. **Use lazy evaluation** for memory efficiency with large datasets
6. **Test async pipelines** properly with await in tests
7. **Add proper error handling** around complex FxTS operations

### ğŸš¨ Critical Pattern:
```typescript
// âœ… THE GOLDEN PATTERN for NestJS Services
const result = await pipe(
  data,
  toAsync,  // Always start with toAsync
  filter(...),
  map(...),
  sortBy(...),
  take(...),
  toArray   // Always end with toArray
);
```

Remember: FxTS promotes immutability, composability, and clear data transformation pipelines. Always prefer functional approaches over imperative ones when working with data transformations.

## ğŸŒŠ Advanced Streaming & Generator Patterns

### ğŸš¨ Critical Issue: `tap` Function with AsyncIterableIterator

**Problem**: When using `tap` with FxTS pipelines, you cannot directly access properties of items in an AsyncIterableIterator.

```typescript
// âŒ BAD: tap on AsyncIterableIterator
const result = await pipe(
  events,
  toAsync,
  map(event => transform(event)),
  tap(event => console.log(event.type)), // ERROR: Property 'type' does not exist
  toArray
);

// âœ… GOOD: Convert to array before tap
const result = await pipe(
  events,
  toAsync,
  map(event => transform(event)),
  toArray,  // Convert to array first
  tap(events => events.forEach(event => 
    console.log(event.type) // Now we can access properties
  ))
);
```

### ğŸ”‘ Key Pattern: Logging in Streaming Pipelines

```typescript
// âœ… EXCELLENT: Proper logging pattern
const uploadEvents = await pipe(
  recentUploads,
  toAsync,
  filter(video => video.uploadedBy !== null),
  map((video): VideoUploadEvent => ({
    id: `upload-${video.idx}-${Date.now()}`,
    type: 'VIDEO_UPLOAD',
    timestamp: new Date(),
    data: { /* ... */ }
  })),
  toArray, // Convert to array before logging
  tap(events => events.forEach(event => 
    this.logger.log(`ğŸ“¹ Video upload event: ${event.data.filename}`)
  ))
);
```

### ğŸŒŠ Generator Functions for Infinite Streams

```typescript
// âœ… EXCELLENT: AsyncGenerator for infinite event streams
async *createEventStream(): AsyncGenerator<StreamEvent, never, unknown> {
  let eventId = 1;
  
  while (true) {
    try {
      // Generate events from real data sources
      const recentData = await this.fetchRecentData();
      const event = await this.generateEvent(eventId++, recentData);
      
      yield event;
      
      // Control flow rate
      await delay(Math.random() * 2000 + 500); // 0.5-2.5 seconds
      
    } catch (error) {
      // Error recovery in streams
      yield {
        id: `error-${eventId++}`,
        type: 'ERROR_EVENT',
        timestamp: new Date(),
        data: { error: error.message }
      };
      
      await delay(5000); // Longer delay on errors
    }
  }
}
```

### ğŸ”„ Backpressure Control in Streaming

```typescript
// âœ… EXCELLENT: Controlled batch processing with backpressure
async processStreamInBackground(): Promise<void> {
  try {
    let eventBuffer: StreamEvent[] = [];
    
    for await (const event of this.createEventStream()) {
      if (!this.isStreaming) break;
      
      eventBuffer.push(event);
      
      // Backpressure: Process when buffer is full
      if (eventBuffer.length >= 10) {
        await this.processBatch(eventBuffer);
        eventBuffer = []; // Clear buffer
      }
    }
  } catch (error) {
    this.logger.error('Stream processing error:', error);
    this.isStreaming = false;
  }
}

private async processBatch(events: StreamEvent[]): Promise<void> {
  await pipe(
    events,
    toAsync,
    chunk(5), // Process in smaller chunks
    map(async (eventChunk) => {
      return await pipe(
        eventChunk,
        toAsync,
        map(async (event) => {
          await delay(100); // Rate limiting
          await this.processEvent(event);
          return event;
        }),
        toArray
      );
    }),
    toArray
  );
}
```

### ğŸ“Š Real-time Window Analysis

```typescript
// âœ… EXCELLENT: Time-based event window analysis
async analyzeEventWindow(events: StreamEvent[]): Promise<WindowAnalysis> {
  return await pipe(
    events,
    toAsync,
    toArray, // Convert for analysis
    
    // Analysis transformation
    (eventArray) => {
      const eventCounts = eventArray.reduce((acc, event) => {
        acc[event.type] = (acc[event.type] || 0) + 1;
        return acc;
      }, {} as Record<string, number>);
      
      const topEventType = Object.entries(eventCounts)
        .sort(([,a], [,b]) => b - a)[0]?.[0] || 'none';
      
      return {
        windowStart: new Date(Date.now() - 60000), // 1 minute window
        windowEnd: new Date(),
        eventCounts,
        totalEvents: eventArray.length,
        eventsPerSecond: eventArray.length / 60,
        topEventType,
        videoUploads: eventCounts['VIDEO_UPLOAD'] || 0,
        userActivities: eventCounts['USER_ACTIVITY'] || 0,
        systemMetrics: eventCounts['SYSTEM_METRIC'] || 0
      };
    }
  );
}
```

### ğŸ¯ Event Stream Sampling

```typescript
// âœ… EXCELLENT: Probabilistic sampling for high-volume streams
async sampleEventStream(events: StreamEvent[], sampleRate: number = 0.1): Promise<StreamEvent[]> {
  return await pipe(
    events,
    toAsync,
    filter(() => Math.random() < sampleRate), // Probabilistic sampling
    toArray,
    tap(sampledEvents => 
      this.logger.log(`ğŸ¯ Sampled ${sampledEvents.length} events from ${events.length} total`)
    )
  );
}
```

### ğŸ” Multi-Criteria Stream Filtering

```typescript
// âœ… EXCELLENT: Complex filtering with multiple criteria
async filterEventStream(
  events: StreamEvent[], 
  filters: {
    eventTypes?: StreamEvent['type'][];
    timeRange?: { start: Date; end: Date };
    userId?: number;
  }
): Promise<StreamEvent[]> {
  return await pipe(
    events,
    toAsync,
    
    // Event type filter
    filter(event => {
      if (filters.eventTypes && !filters.eventTypes.includes(event.type)) {
        return false;
      }
      return true;
    }),
    
    // Time range filter
    filter(event => {
      if (filters.timeRange) {
        const eventTime = event.timestamp.getTime();
        const startTime = filters.timeRange.start.getTime();
        const endTime = filters.timeRange.end.getTime();
        return eventTime >= startTime && eventTime <= endTime;
      }
      return true;
    }),
    
    // User ID filter
    filter(event => {
      if (filters.userId && event.data.userId) {
        return event.data.userId === filters.userId;
      }
      return true;
    }),
    
    toArray,
    tap(filteredEvents => 
      this.logger.log(`ğŸ” Filtered ${filteredEvents.length} events from ${events.length} total`)
    )
  );
}
```

### ğŸ“ˆ Event Trend Analysis

```typescript
// âœ… EXCELLENT: Time-series trend analysis
async analyzeEventTrends(events: StreamEvent[]): Promise<TrendAnalysis> {
  return await pipe(
    events,
    toAsync,
    
    // Add time-based grouping info
    map(event => ({
      ...event,
      hour: event.timestamp.getHours(),
      minute: Math.floor(event.timestamp.getMinutes() / 10) * 10 // 10-minute buckets
    })),
    
    toArray,
    
    // Trend analysis
    (eventArray) => {
      const hourlyTrends = eventArray.reduce((acc, event) => {
        const key = `${event.hour}:${event.minute}`;
        acc[key] = (acc[key] || 0) + 1;
        return acc;
      }, {} as Record<string, number>);
      
      const typeTrends = eventArray.reduce((acc, event) => {
        acc[event.type] = (acc[event.type] || 0) + 1;
        return acc;
      }, {} as Record<string, number>);
      
      return {
        hourlyTrends,
        typeTrends,
        totalEvents: eventArray.length,
        peakHour: Object.entries(hourlyTrends)
          .sort(([,a], [,b]) => b - a)[0]?.[0] || 'none',
        mostActiveType: Object.entries(typeTrends)
          .sort(([,a], [,b]) => b - a)[0]?.[0] || 'none'
      };
    }
  );
}
```

### ğŸš¦ Stream Rate Control Patterns

```typescript
// âœ… EXCELLENT: Rate-limited stream processing
async processWithRateLimit<T>(
  items: T[],
  processor: (item: T) => Promise<void>,
  rateLimit: number = 100 // ms between items
): Promise<void> {
  await pipe(
    items,
    toAsync,
    map(async (item) => {
      await processor(item);
      await delay(rateLimit); // Rate control
      return item;
    }),
    toArray
  );
}

// âœ… EXCELLENT: Chunked processing with delays
async processLargeDataset<T>(items: T[], chunkSize: number = 10): Promise<void> {
  await pipe(
    items,
    toAsync,
    chunk(chunkSize),
    map(async (chunk) => {
      // Process chunk
      const results = await pipe(
        chunk,
        toAsync,
        map(async (item) => await this.processItem(item)),
        toArray
      );
      
      // Delay between chunks
      await delay(1000);
      return results;
    }),
    toArray
  );
}
```

## ğŸš¨ Streaming Anti-Patterns to Avoid

### âŒ Anti-Pattern #6: Logging Before Array Conversion
```typescript
// âŒ BAD: Trying to log AsyncIterableIterator properties
const result = await pipe(
  data,
  toAsync,
  map(transform),
  tap(item => console.log(item.property)), // ERROR: Property access on AsyncIterableIterator
  toArray
);

// âœ… GOOD: Log after converting to array
const result = await pipe(
  data,
  toAsync,
  map(transform),
  toArray,
  tap(items => items.forEach(item => console.log(item.property)))
);
```

### âŒ Anti-Pattern #7: Infinite Streams Without Control
```typescript
// âŒ BAD: No rate control or error handling
async *badInfiniteStream() {
  while (true) {
    yield await fetchData(); // No delay, no error handling
  }
}

// âœ… GOOD: Controlled infinite stream
async *goodInfiniteStream() {
  while (true) {
    try {
      const data = await fetchData();
      yield data;
      await delay(1000); // Rate control
    } catch (error) {
      console.error('Stream error:', error);
      await delay(5000); // Longer delay on error
    }
  }
}
```

### âŒ Anti-Pattern #8: No Backpressure Management
```typescript
// âŒ BAD: Processing without backpressure control
for await (const event of eventStream) {
  await processEvent(event); // Can overwhelm system
}

// âœ… GOOD: Batched processing with backpressure
let buffer: Event[] = [];
for await (const event of eventStream) {
  buffer.push(event);
  
  if (buffer.length >= 10) {
    await processBatch(buffer);
    buffer = [];
  }
}
```

## ğŸ“‹ Streaming Patterns Summary

### ğŸ”‘ Critical Streaming Rules:
1. **Always convert to array before `tap`** when accessing item properties
2. **Use generators for infinite streams** with proper error handling and rate control
3. **Implement backpressure** in high-volume stream processing
4. **Add delays for rate limiting** to prevent system overload
5. **Use chunking for memory efficiency** when processing large datasets
6. **Implement proper error recovery** in infinite streams
7. **Monitor and log stream health** with proper metrics

### ğŸš¨ Golden Streaming Pattern:
```typescript
// âœ… THE GOLDEN STREAMING PATTERN
async processEventStream(): Promise<void> {
  try {
    for await (const event of this.createEventStream()) {
      const processed = await pipe(
        [event], // Convert single event to array
        toAsync,
        filter(this.isValidEvent),
        map(this.transformEvent),
        toArray,
        tap(events => this.logEvents(events))
      );
      
      await this.persistEvents(processed);
      await delay(100); // Rate control
    }
  } catch (error) {
    this.handleStreamError(error);
  }
}
```

Remember: Streaming with FxTS requires careful attention to type safety, rate control, and error handling. Always prioritize system stability over processing speed.




